{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Every week, you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NOT EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `tsds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Week 3: Data modelling\n",
    "\n",
    "*Thursday, February 22, 2018*\n",
    "\n",
    "In this part of today's session you will be working with implementing machine learning models. \n",
    "- Exercise 3.1: supervised regression\n",
    "- Exercise 3.2: supervised classification\n",
    "- Exercise 3.3: unsupervised learning with k-means and principal compoments\n",
    "\n",
    "We begin with loading the standard packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1: Predicting tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part of the exercise we will implement a machine learning model for predicting tips. We will use the same data as in Exercise 1.\n",
    "\n",
    "> **Ex. 3.1.0**: Load the tips data from Seaborn. This can be done with `load_dataset` giving it `'tips'` as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tips = sns.load_dataset('tips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structuring the data\n",
    "\n",
    "We have loaded a simple dataset. We are interested in modelling the tips as a function of the dining characteristics (bill size, participants, time). Thus `tips` is our labelled data which we will use in our supervised machine learning problem.\n",
    "\n",
    "Although data is more or less structured we need to tweak it a little bit. Linear models using regularization need data that is normalized. Furthermore linear models need categorical data to be replaced by dummy variables which linear regression models can handle.\n",
    "\n",
    "> **Ex. 3.1.1**: Use  L2 normalization (i.e. mean zero, one std. dev.) on the numeric columns, i.e. 'total_bill', 'tip', 'size'. Convert categorical/discrete data to dummy variables. Call the final dataset 'data'.\n",
    "\n",
    "> *Hint 1*: `sklearn.preprocessing` has a helpful method called `normalize`.\n",
    "\n",
    "> *Hint 2*: the `get_dummies` method in pandas may be useful with the keyword argument `drop_first=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.1.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our model we are interested in evaluating on a different sample from where we estimate the model. That is, we split into a training data set for estimation and a test data set for evaluation.\n",
    "\n",
    "The size of the training dataset relative to the test data set depends on the number of observations you have. The more observations you have, the larger you can make the training dataset. Because we have a small dataset, i.e. number of observation `<`1000, we should have a large share of test data of around 30 pct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.1.2**:  Specify the exogenous variable **y** as `'tips'` and **X** as the remaining variables.  Then split the dataset into a training set and a test set. The respective sizes of the train and test data should be 70 pct. and 30 pct.\n",
    "\n",
    "> *Hint*: try using the `sample` method available to dataframes. Use `random_state` = 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a model of tips\n",
    "\n",
    "Having prepared the data we can now begin the modelling. We want to compare the models of ordinary least squares (OLS) against the Lasso. Note that in `sklearn` $\\lambda$ is called $alpha$; the reason is that in Python lambda, and programming in general, lambda refers to a specific type of function.\n",
    "\n",
    "> **Ex. 3.1.3**: Use the training data to fit models with the explanatory variables. Try both OLS and the Lasso with $alpha=0.001$. \n",
    "\n",
    "> *Hint*: OLS is called `LinearRegression` in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.1.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Ex. 3.1.4**: Compute the predicted tips. Evaluate the models on the test data. What is the root mean squared error (RMSE)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.1.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "\n",
    "One of the main challenges of machine learning is creating meaningful variables may be relevant. The simplest way to do that is attempt different interactions between the variables.\n",
    "\n",
    "> **Ex. 3.1.5**: Create interactions between all explanatory variables.\n",
    "\n",
    "> *Hint*: Try the `PolynomialFeatures` method in sklearn's preprocessing submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.1.6**: Repeat the above comparison of OLS and Lasso. What happens to RMSE if we set $alpha=10^{-6}$?  Does Lasso improve the test predictions? Does the $\\alpha$ hyperparamater matter?\n",
    "\n",
    "> Note you need to split the quadratic feature set as well into test and training - use the same split indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.1.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.1.7**: What happens to the Lasso if we compute polynomial features for higher orders of `n`? Try n=1,2,3,...,10. What would happen to OLS if we tried to estimate for higher and higher n - is this feasible? How many variables do we have in the features set for n=10? Do we have more variables than observations?\n",
    "\n",
    "> *Hint*: We can reuse the above code using a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.1.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.1.8**: Use the ridge regression to make a predictive model using the quadratic feature set. Use $alpha=10^{-3}$. How does it perform in  terms of RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.1.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2: Predicing high, low wage\n",
    "\n",
    "Download the wage dataset available in the data folder at:\n",
    "\n",
    " https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex 3.2.1**: Load both train and test data. Ensure na_values are parsed correctly, assign column names and check if some rows should be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.2.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex 3.2.2:** Structure the data using the following steps:\n",
    "1. Drop columns that have no relevant information (e.g. encoded both as number and as text).\n",
    "2. Use L2 normalization (ie. mean zero, unit std. dev.) on the continuous variables (e.g. 'age'). \n",
    "3. Ensure wage is encoded correctly.\n",
    "4. Add dummies for categorical or string variables.\n",
    "5. Drop rows with any missing data\n",
    "6. Partition into y,X as well as test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.2.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating and valuating classification models\n",
    "\n",
    "> **Ex 3.2.3:** Estimate logistic regression for $C$ = $10^{-2}, 10^{-1}, ..., 10^{3}, 10^{4}$. Read the documentation of the logistic regression; what is the relationship between $alpha$ and $C$? What is the overall accuracy of the models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.2.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating classification models the overall accuracy of the model is rarely a good indicator of the errors. Often we are interested in whether we make Type I or II errors. This can be measured by precision and recall. If you are unsure what the precision, recall and F1 is, Wikipedia has a good overview that you should read [here](https://en.wikipedia.org/wiki/Precision_and_recall). \n",
    "\n",
    "> **Ex 3.2.4:** For each of the estimated models compute precision, recall and F1 for the class `wage_above_50k==1`.\n",
    "\n",
    "> *Hint*: Although computation is straightforward it may be easier to use scikit learn's built in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.2.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3: Unsupervised learning \n",
    "\n",
    "In this exercise we will work with unlabelled data. We will try out to fundamental approaches to unsupervised learning. We will be using one of the classic datasets in machine learning, the `iris` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Ex. 3.3.0**: Load the `iris` data from Seaborn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.3.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### K-means clustering\n",
    "\n",
    "Data clustering is a method to divide observations into groups where points are similar to one another. In particular the clustering algorithms aims to provide the same label to similar observations. This approach has the advantage that it can detect underlying groups without needing access to labelled data. Thus it may be used as feature engineering.\n",
    "\n",
    "The k-means clustering algorithm outputs for a given input set $k$ points that serves as clustering centers. Points are assigned to cluster that they are closest to in Euclidian space. The assignment procedure uses an iterative optimization procedure. Therefore it needs a starting point and thus depend on the initial (random) guess. \n",
    "\n",
    "To get an idea of how the k-means algorithm works try out the implementation visualized [here](http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Ex. 3.3.1**: Compute a K-means cluster. Use three clusters.\n",
    "\n",
    "> *Hint*: the submodule `cluster` in sklearn has a method called `KMeans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.3.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Ex. 3.3.2**: Make a scatterplot of with `petal_length` and `petal_width` as respectively x and y axis. Color the points with species. Make another plot using the computed k-means. Would the k-means label provide a good feature for predicing `species`?\n",
    "\n",
    "> *Hint:* try seaborn's `lmplot` without fitting a regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.3.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality reduction and principal components\n",
    "\n",
    "In datasets with many variables it can often be difficult to distinguish which ones are relevant. Especially if the variables are highly correlated. One approach is to reduce the number of dimensions by finding one or more variables that explains large parts of the data. One approach is linear and called principal components analysis (**PCA**). The aim of PCA is to find variables that explain as much of the variation in the data as possible.\n",
    "\n",
    "> **Ex. 3.3.3**: Estimate a PCA model using the four features: `'sepal_length', 'sepal_width', 'petal_length', 'petal_width'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.3.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 3.3.4**: How much variance is explained by the respective components, i.e. eigenvectors? Try to plot the variance explained by the eigenvalues where eigenvalues are in non-increasing order; this is known as a scree plot. What does the scree plot look like? Make a plot of the species using the two first components. Does the new dimension help distinguish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 3.3.4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
