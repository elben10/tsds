{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Focus of today\n",
    "* Recap of the essentials of web scraping.\n",
    "    * Navigating html, reliable connection, parsing, storing, logging.\n",
    "\n",
    "* Showing you some tricks of the trade\n",
    "* Hands on with APIs\n",
    "\n",
    "... \n",
    "* Pointing to the skills you need to master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation\n",
    "<a href=\"https://imgflip.com/i/24m2p1\"><img src=\"https://i.imgflip.com/24m2p1.jpg\" title=\"made at imgflip.com\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Main take-away\n",
    "* Central problem you are facing is *ensuring* and *demonstrating* **data quality** by:\n",
    "    * Logging, \n",
    "        * the scraping process\n",
    "        * the parse\n",
    "    * ensuring reproducibility (storing raw data)        \n",
    "    * inspections (random selections of your parse)\n",
    "    * exploratory data analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quick setup - connecting and legal notice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ethics / Legal Issues\n",
    "* If a regular user can’t access it, we shouldn’t try to get it (That is considered hacking)https://www.dr.dk/nyheder/penge/gjorde-opmaerksom-paa-cpr-hul-nu-bliver-han-politianmeldt-hacking. \n",
    "* Don't hit it to fast: Essentially a DENIAL OF SERVICE attack (DOS). [Again considered hacking](https://www.dr.dk/nyheder/indland/folketingets-hjemmeside-ramt-af-hacker-angreb). \n",
    "* Add headers stating your name and email with your requests to ensure transparency. \n",
    "* Be careful with copyrighted material.\n",
    "* Fair use (don't take everything)\n",
    "* If monetizing on the data, be careful not to be in direct competition with whom you are taking the data from.\n",
    "* Who are you collecting data about?\n",
    "    * Political groups, minorities or organizations, or all at the same time?\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Connecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'python-requests/2.18.4', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'email': 'youremail', 'name': 'name'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transparent scraping\n",
    "import requests\n",
    "#response = requests.get('https://www.google.com')\n",
    "session = requests.session()\n",
    "session.headers['email'] = 'youremail' \n",
    "session.headers['name'] = 'name'\n",
    "session.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A quick tip is that you can change the user agent to a cellphone to obtain more simple formatting of the html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Control the pace of your calls\n",
    "import time\n",
    "def ratelimit():\n",
    "    \"A function that handles the rate of your calls.\"\n",
    "    time.sleep(1) # change to design more clever schemes for handling the rate of calls.\n",
    "# Reliable requests\n",
    "def request(url,iterations=10,exceptions=(Exception)):\n",
    "    \"\"\"This module ensures that your script does not crash from connection errors.\n",
    "        iterations : Define number of iterations before giving up. \n",
    "        exceptions: Define which exceptions you accept, default is all. \n",
    "    \"\"\"\n",
    "    for iteration in range(iterations):\n",
    "        try:\n",
    "            # add ratelimit function call here\n",
    "            ratelimit() # !!\n",
    "            response = session.get(url)\n",
    "            return response # if succesful it will end the iterations here\n",
    "        except exceptions as e: #  find exceptions in the request library requests.exceptions\n",
    "            print(e) # print or log the exception message.\n",
    "    return None # your code will purposely crash if you don't create a check function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Interactive browsing. \n",
    "# Need to download geckodrivers (https://github.com/mozilla/geckodriver/releases) and install selenium first\n",
    "from selenium import webdriver\n",
    "path2gecko = '/Users/axelengbergpallesen/Downloads/geckodriver' # define path to your geckodriver\n",
    "browser = webdriver.Firefox(executable_path=path2gecko)\n",
    "browser.get('https://www.facebook.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Essentials of webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Essentials (1): Reading html\n",
    "* Using the Inspector\n",
    "    * locating paths to elements: e.g css selectors\n",
    "    * structure of the html-tree\n",
    "    * uncover hidden information e.g. extra meta data.\n",
    "    * Find direct links to the underlying database. Visit (https://www.jobnet.dk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "url = ''\n",
    "request(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Essentials(2): Navigation\n",
    "- http hacking: Visit https://www.jobindex.dk\n",
    "- crawling and following links\n",
    "- interactions \n",
    "    * send keys: e.g. login \n",
    "    * clicks\n",
    "    * scroll-down (https://stackoverflow.com/questions/20986631/how-can-i-scroll-a-web-page-using-selenium-webdriver-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# hardcoded interaction\n",
    "css_sel = ''\n",
    "element = browser.find_element_by_css_selector(css_sel)\n",
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Essentials(3): Parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scraping', 'is', 'easy', '?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # generic pattern search\n",
    "from bs4 import BeautifulSoup # navigating the html tree\n",
    "'Scraping,2,is,3,easy,6,?'.split(',')[::2] # good old split function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](http://www.tankonyvtar.hu/hu/tartalom/tamop412A/2011_0015_kliens_oldali_technologiak_EN/files/4018.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://it.jobindex.dk/jobsoegning?maxdate=20180214&mindate=20171101&jobage=archive'\n",
    "response = request(url)\n",
    "soup = BeautifulSoup(response.text,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets find the section with the results.\n",
    "results = soup.findAll('div',{'class':'results  component--default'})\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "** shit! ** it is dynamically generated. \n",
    "\n",
    "Now we have the possibility of using selenium or inpecting the network acitivty / following the html build instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response is OK\n"
     ]
    }
   ],
   "source": [
    "url = 'https://it.jobindex.dk/jobsoegning.json?q=&jobage=archive&mindate=20171101&maxdate=20180214&page=0'\n",
    "response = request(url)\n",
    "if response.ok:\n",
    "    \n",
    "    print('response is OK')\n",
    "    soup = BeautifulSoup(response.json()['result_list_box_html'],'lxml')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "results = soup.find('div',{'class':'results component--default'})\n",
    "for child in results.findAll('div'):\n",
    "    break\n",
    "    # getting links.\n",
    "    relative_link = child.a['data-click']\n",
    "\n",
    "    # getting headlines.\n",
    "    headline = child.b.text\n",
    "    # company\n",
    "    company = child.p.a.b.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Essentials(4): Storing, and formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "open(filename,'w')\n",
    "#formats\n",
    "#raw,pickle,json, csv\n",
    "f.write\n",
    "json.dump(), json.dumps\n",
    "csv.writer\n",
    "pd.to_csv,pd.to_pickle,pd.to_feather \n",
    "# for arbitrary python objects\n",
    "pickle.dump\n",
    "# Databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "! mkdir jobindex_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# paging the first 5\n",
    "for page_n in range(5):\n",
    "    url = 'https://it.jobindex.dk/jobsoegning.json?q=&jobage=archive&mindate=20171101&maxdate=20180214&page=%d'%page_n\n",
    "    response = request(url)\n",
    "    # store it.\n",
    "    if response.ok:\n",
    "        path = 'jobindex/jobindex_page_%d'%page_n\n",
    "        f = open(path,'w')\n",
    "        f.write(response.text)\n",
    "        f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Essentials (5): Logging and Quality Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "response = request('https://www.google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# setting up the logfile\n",
    "! mkdir logs\n",
    "log_id = 0 \n",
    "logfile = open('logs/test_log','w') \n",
    "log_header = ['log_id','status_code','timestamp','length','output_file']  # \n",
    "logfile.write(','.join(log_header)) # comment this out after the first write\n",
    "logfile.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13236"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logfile = open('logs/test_log','a') \n",
    "import time\n",
    "def log(response,output_path,logfile=logfile):\n",
    "    \"A simple example of a logging function\"\n",
    "    global log_id\n",
    "    # give it an id\n",
    "    log_id += 1\n",
    "    status_code = response.status_code\n",
    "    timestamp = time.time()\n",
    "    length = len(response.text)\n",
    "    # Header 'log_id','status_code','timestamp','length','output_file'\n",
    "    logfile.write('%d,%d,%r,%d,%s'%(log_id,status_code,timestamp,length,output_path))\n",
    "    # add a write function here\n",
    "    \n",
    "! mkdir test_scrape\n",
    "log(response,'test_scrape/google.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quality of the parse\n",
    "* Number of missing values accross different Variables, and accross different sites or subsections of a site.\n",
    "* If temporal data: quality accross time.\n",
    "* Profile missing values\n",
    "    * Might be related to time of day, rate of calls, length of the response, or other more complex factors logged.\n",
    "* Do many random samples of the parse and checks of the final dataframe or dataset.\n",
    "    * E.g. do random sample in pandas like this: df.sample(frac=0.1).head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scraping the Cryptomarkets to investigate rate of success, bots and scams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise (1)\n",
    "Visit following page: https://coinmarketcap.com/\n",
    "\n",
    "The page allows you to investigate the ***\"big\"*** players of the cryptomarket. \n",
    "\n",
    "** Navigation **\n",
    "- Find a way to navigate the page and extract links to the blockchain companies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise (2)\n",
    "Now visit one of the links and see what data you can get.\n",
    "\n",
    "**Backdoors**\n",
    "- Look at the network monitor and locate the link to the file with the historical trading data (i.e. the graph displayed). \n",
    "- See how the link is constructed, and make a loop that collects and stores the first 5 companies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise (3)\n",
    "** HTML parsing **\n",
    "\n",
    "Now we need to visit the pages and collect data stored in the html.\n",
    "\n",
    "Use the links collected before and store the first 5 raw html responses\n",
    "\n",
    "* Design a parser that collects the Twitter handle of each company - from the Social Media Feed option (+whatever meta data you like) - and apply it to the raw html.\n",
    "    - Note not all companies has a twitter handle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# APIs\n",
    "For fast, efficient and ***reliable*** data collection.\n",
    "\n",
    "Only catch is that they control the amounts, and which endpoints you can collect \n",
    "\n",
    "also they **change**.\n",
    "    - e.g. facebook cancelled \n",
    "        - querying friendship relations (without having using signing up to your app), \n",
    "        - group activity without admin rights, \n",
    "        - and most recently the ability to trace public activity (likes and comments) without admin rights.\n",
    "    - twitter (and more recently facebook) will not let you collect all historic activity --> streaming data.\n",
    "\n",
    "Begins with reading the docs... \n",
    "- getting authentification - creating apps, getting and renewing tokens - \n",
    "- building queries.\n",
    "- ratelimiting and pagination.\n",
    "\n",
    "Often comes in the Json format. --> nested dictionaries and lists.\n",
    "\n",
    "Example: Explore the facebook api here: https://developers.facebook.com/tools/explorer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## APIS: Collect data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# XXX: Go to http://dev.twitter.com/apps/new to create an app and get values\n",
    "# for these credentials that you'll need to provide in place of these\n",
    "# empty string values that are defined as placeholders.\n",
    "# See https://dev.twitter.com/docs/auth/oauth for more information \n",
    "# on Twitter's OAuth implementation\n",
    "\n",
    "CONSUMER_KEY=\"\"\n",
    "CONSUMER_SECRET=\"\"\n",
    "OAUTH_TOKEN=\"\"\n",
    "OAUTH_TOKEN_SECRET=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#pickle.dump([CONSUMER_KEY,CONSUMER_SECRET,OAUTH_TOKEN,OAUTH_TOKEN_SECRET],open('twitter_credentials.pkl','wb'))\n",
    "CONSUMER_KEY,CONSUMER_SECRET,OAUTH_TOKEN,OAUTH_TOKEN_SECRET = pickle.load(open('twitter_credentials.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# answer from https://stackoverflow.com/questions/33308634/how-to-perform-oauth-when-doing-twitter-scrapping-with-python-requests\n",
    "from requests_oauthlib import OAuth1\n",
    "#url = 'https://api.twitter.com/1.1/account/verify_credentials.json' # verify that your authorization works.\n",
    "#url = 'https://api.twitter.com/1.1/statuses/user_timeline.json?screen_name=singularity_net&count=5000' # get timeline\n",
    "\n",
    "# Description of the call https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference/get-followers-list\n",
    "url = 'https://api.twitter.com/1.1/followers/list.json?cursor=-1&screen_name=%s&count=5000'%twitter_name\n",
    "\n",
    "auth = OAuth1(CONSUMER_KEY, CONSUMER_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "response = requests.get(url, auth=auth).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define container to store the responses.\n",
    "responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 "
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'next_cursor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-e748b2e4110a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'next_cursor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://api.twitter.com/1.1/followers/list.json?cursor=%d&screen_name=singularity_net&count=5000'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'next_cursor'"
     ]
    }
   ],
   "source": [
    "cursor = -1 # start with -1\n",
    "url = 'https://api.twitter.com/1.1/followers/list.json?cursor=%d&screen_name=singularity_net&count=5000' %cursor\n",
    "auth = OAuth1(CONSUMER_KEY, CONSUMER_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
    "response = requests.get(url, auth=auth).json()\n",
    "responses.append(response)\n",
    "count = 0\n",
    "while cursor!=0:\n",
    "    time.sleep(1)\n",
    "    cursor = response['next_cursor']\n",
    "    url = 'https://api.twitter.com/1.1/followers/list.json?cursor=%d&screen_name=singularity_net&count=5000'%cursor\n",
    "    response = requests.get(url, auth=auth).json()\n",
    "    # dump response\n",
    "    responses.append(response)\n",
    "    count+=1\n",
    "    if count%10==0:\n",
    "        print(count,end=' ')\n",
    "#response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'errors': [{'code': 88, 'message': 'Rate limit exceeded'}]}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Packages: python-twitter and tweepy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise(4)\n",
    "- Collect the link to each of the crypto-companies' github page (Source Code) and get data from the api.\n",
    "    * E.g. Get the contributors of the repository. - Count the number of unique contributors.\n",
    "    * Get events. \n",
    "    * Get forks.\n",
    "    \n",
    "\n",
    "## GITHUB API\n",
    "Check the docs:\n",
    "https://developer.github.com/v3/\n",
    "\n",
    "### Practice with some calls###\n",
    "The github api is nice to work with because it provides links to further calls that can be made.\n",
    "* This will list basic queries: https://api.github.com/\n",
    "* Play around with it: e.g. https://api.github.com/repos/singnet/singnet/contributors \n",
    "\n",
    "### Read about paging\n",
    "here: https://developer.github.com/v3/guides/traversing-with-pagination/\n",
    "### Rate limits\n",
    "For practice you can make calls without authorization. However if you need large amounts of data you need an authorization token.\n",
    "This means the standard procedure: https://developer.github.com/apps/building-oauth-apps/authorization-options-for-oauth-apps/\n",
    "    * Create an app here (under developers settings): https://github.com/settings/, collect client_id and client_secret for the app.\n",
    "    * Let a user (yourself) Authorize your app. Using the specified procedure described https://github.com/login/oauth/authorize?client_id=(indsæt_client_id).\n",
    "    * Get the code returned (look at the url in the top of the browser).\n",
    "    * Use the code, client_id, client_secret to request an access_token: 'https://github.com/login/oauth/access_token?code=%s&client_id=%s&client_secret=%s'%(code,client_id,client_secret)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ressource a guide to authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# look at the guide for making authorized calls:\n",
    "# https://developer.github.com/apps/building-oauth-apps/authorization-options-for-oauth-apps/\n",
    "client_id = ''#found in the settings of your newly generated test app\n",
    "client_secret = '' # also found in the settings of your app\n",
    "code = ''# found using the call  https://github.com/login/oauth/authorize?client_id=%s'%client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#pickle.dump([client_id,client_secret,code],open('github_credentials.pkl','wb'))\n",
    "client_id,client_secret,code = pickle.load(open('github_credentials.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# as described in the docs https://developer.github.com/apps/building-oauth-apps/authorization-options-for-oauth-apps/\n",
    "url = 'https://github.com/login/oauth/access_token?code=%s&client_id=%s&client_secret=%s'%(code,client_id,client_secret)\n",
    "response = request(url)\n",
    "token = response.text.split('access_token=')[0].split('&')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#pickle.dump(token,open('gittoken','wb'))\n",
    "token = pickle.load(open('gittoken','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Solutions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# list possible calls: https://api.github.com/\n",
    "\n",
    "# check your ratelimit: 'https://api.github.com/rate_limit?access_token=%s'%token\n",
    "# interesting calls\n",
    "# https://api.github.com/repos/singnet/singnet/contributors # contributors to a project, create a network of co-contributors.\n",
    "# 'https://api.github.com/repos/singnet/singnet/forks'\n",
    "#'https://api.github.com/repos/singnet/singnet/events' # get all events (watch and fork, pull, push and commit events), are they coordinated?\n",
    "## are they from bots. e.g. dmonroe3 who follows 5 blockchain things.\n",
    "## Also includes push events and messages.\n",
    "# 'https://api.github.com/users/dmonroe3/followers # get the followers of a user\n",
    "# https://api.github.com/users/thinxtank # get basic info about a user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### paging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<https://api.github.com/repositories/100654282/forks?page=2>; rel=\"next\", <https://api.github.com/repositories/100654282/forks?page=3>; rel=\"last\"'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url ='https://api.github.com/repos/singnet/singnet/forks'\n",
    "response = request(url)\n",
    "response.headers['Link'] # Here you can read how many pages to page. read here: https://developer.github.com/v3/guides/traversing-with-pagination/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perspectives\n",
    "## Regex\n",
    "For cleaning, further extraction of information from html and text, you need to learn Regular expressions --> More to come in the future sessions. \n",
    "\n",
    "    - For now. Go play around at https://www.regexr.com.\n",
    "\n",
    "or do the following exercises:\n",
    "- Locate the link to the website of each of the blockchain companies. \n",
    "- Try to find the number of doctors displayed on the website of each project (frontpage or crawl the whole map, while your at it see if you can do some other stats about the team.). Match the name after the Dr. prefix.\n",
    "- Find the numbers thrown around on each website: $9234 million / trillion / thousands. \n",
    "    - Find the previous word before the number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perspectives (2)\n",
    "## Common scenarios\n",
    "Tabular data from html.\n",
    "    - Beautifulsoup: Google is your friend (e.g. https://stackoverflow.com/questions/23377533/python-beautifulsoup-parsing-table)\n",
    "    - Pandas: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html\n",
    "\n",
    "Interactions(clicking, scrolling, logins etc):\n",
    "    * Use selenium.\n",
    "        - Select element (using css-selector path or search as in beautifulsoup)\n",
    "            - element.click()\n",
    "            - element.send_keys()\n",
    "        - scrolling (google \"scroll to the bottom selenium python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perspectives(3)\n",
    "### Reliability\n",
    "EDA"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
